{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa9tVx7T7k2nd9iR6KtaBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ondennis03/AAI2026/blob/main/CodingExercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNXlqAX_yeJQ",
        "outputId": "b08ab540-cecd-4be1-aa89-40c448aec7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Part 1: House Price Prediction ===\n",
            "Test MAE: $32,165\n",
            "Test R^2: 0.858\n",
            "\n",
            "Predicted price for 2000 sq ft in Downtown: $633,655\n",
            "\n",
            "Intercept: $206,521.32\n",
            "\n",
            "Coefficients:\n",
            "  location_Rural: -165,505.19\n",
            "  location_Suburb: -82,482.15\n",
            "  square_footage: 213.57\n",
            "\n",
            "NOTE: Location coefficients are relative to baseline location = 'Downtown'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Part 1\n",
        "# Data source: ChatGPT prompts (300 rows)\n",
        "# Data source file name: house_prices_with_location_v2\n",
        "# location column was randomly generated with categories: Downtown, Rural, Suburb\n",
        "\n",
        "# Load data (use the correct filename/path where YOU saved it)\n",
        "df = pd.read_csv(\"house_prices_with_location_v2.csv\")\n",
        "\n",
        "# Features + target (names match the CSV exactly)\n",
        "X = df[[\"square_footage\", \"location\"]]\n",
        "y = df[\"price\"]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Encode location (drop first category so location effects are relative to the baseline)\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"loc\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"), [\"location\"])\n",
        "    ],\n",
        "    remainder=\"passthrough\"  # keeps square_footage\n",
        ")\n",
        "\n",
        "# Pipeline: preprocessing + linear regression\n",
        "model = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"regressor\", LinearRegression())\n",
        "])\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"=== Part 1: House Price Prediction ===\")\n",
        "print(f\"Test MAE: ${mae:,.0f}\")\n",
        "print(f\"Test R^2: {r2:.3f}\")\n",
        "\n",
        "# Predict required example\n",
        "new_house = pd.DataFrame([{\"square_footage\": 2000, \"location\": \"Downtown\"}])\n",
        "pred_price = model.predict(new_house)[0]\n",
        "print(f\"\\nPredicted price for 2000 sq ft in Downtown: ${pred_price:,.0f}\")\n",
        "\n",
        "# Print coefficients (impact of each feature)\n",
        "ohe = model.named_steps[\"preprocess\"].named_transformers_[\"loc\"]\n",
        "location_feature_names = ohe.get_feature_names_out([\"location\"])\n",
        "\n",
        "# Because remainder=\"passthrough\", square_footage is appended after encoded columns\n",
        "feature_names = list(location_feature_names) + [\"square_footage\"]\n",
        "\n",
        "coeffs = model.named_steps[\"regressor\"].coef_\n",
        "intercept = model.named_steps[\"regressor\"].intercept_\n",
        "\n",
        "print(\"\\nIntercept:\", f\"${intercept:,.2f}\")\n",
        "print(\"\\nCoefficients:\")\n",
        "for name, c in zip(feature_names, coeffs):\n",
        "    print(f\"  {name}: {c:,.2f}\")\n",
        "\n",
        "# Quick interpretation helper (baseline is the dropped category)\n",
        "baseline_location = ohe.categories_[0][0]  # the first category dropped by OneHotEncoder\n",
        "print(f\"\\nNOTE: Location coefficients are relative to baseline location = '{baseline_location}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Part 2\n",
        "# Data source: ChatGPT prompts (500 rows)\n",
        "# Data source file name: customer_churn_dataset\n",
        "# Columns: age, monthly_usage, purchase_amount, customer_service_calls, region, churn\n",
        "\n",
        "# 1) Load dataset\n",
        "df = pd.read_csv(\"customer_churn_dataset.csv\")\n",
        "\n",
        "# 2) Ensure required columns exist\n",
        "required_cols = [\"age\", \"monthly_usage\", \"purchase_amount\", \"customer_service_calls\", \"region\", \"churn\"]\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Drop rows with missing required values (basic cleaning step)\n",
        "df = df.dropna(subset=required_cols)\n",
        "\n",
        "# 3) Features / target\n",
        "X = df[[\"age\", \"monthly_usage\", \"purchase_amount\", \"customer_service_calls\", \"region\"]]\n",
        "y = df[\"churn\"]\n",
        "\n",
        "# 4) Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 5) Preprocessing (CHECKLIST ITEMS)\n",
        "numeric_features = [\"age\", \"monthly_usage\", \"purchase_amount\", \"customer_service_calls\"]\n",
        "categorical_features = [\"region\"]\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numeric_features),                       # ✅ StandardScaler\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)  # ✅ OneHotEncoder\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 6) Model (CHECKLIST ITEM)\n",
        "model = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Train logistic regression model ✅\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 7) Evaluate (not required by your checklist, but good practice)\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]  # churn probability ✅\n",
        "\n",
        "print(\"=== Part 2: Customer Churn Prediction ===\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, zero_division=0):.3f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred, zero_division=0):.3f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba):.3f}\")\n",
        "print(\"\\nConfusion Matrix [ [TN FP] [FN TP] ]:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 8) Predict churn probability for a new customer + classify using 0.5 threshold ✅\n",
        "new_customer = pd.DataFrame([{\n",
        "    \"age\": 28,\n",
        "    \"monthly_usage\": 12,\n",
        "    \"purchase_amount\": 70,\n",
        "    \"customer_service_calls\": 3,\n",
        "    \"region\": \"East\"\n",
        "}])\n",
        "\n",
        "new_proba = model.predict_proba(new_customer)[:, 1][0]  # ✅ churn probability output\n",
        "threshold = 0.5\n",
        "new_class = int(new_proba >= threshold)                  # ✅ 0.5 threshold classification\n",
        "\n",
        "print(\"\\nNew customer churn probability:\", f\"{new_proba:.3f}\")\n",
        "print(f\"At-risk (threshold {threshold})? churn =\", new_class)\n",
        "\n",
        "# 9) Interpretation (CHECKLIST ITEMS)\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- The churn probability is the model’s estimate of the chance that this customer will churn.\")\n",
        "print(\"  Example: 0.70 means about a 70% chance the customer will leave.\")\n",
        "print(\"- Businesses can use this to reduce churn by targeting high-risk customers (probability >= 0.5)\")\n",
        "print(\"  with retention actions like discounts, proactive support outreach, plan changes, or loyalty offers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdyV489HrG5o",
        "outputId": "8d019173-d43d-4f1f-cbfd-f214813d62a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Part 2: Customer Churn Prediction ===\n",
            "Accuracy:  0.760\n",
            "Precision: 0.760\n",
            "Recall:    0.760\n",
            "ROC AUC:   0.847\n",
            "\n",
            "Confusion Matrix [ [TN FP] [FN TP] ]:\n",
            "[[38 12]\n",
            " [12 38]]\n",
            "\n",
            "New customer churn probability: 0.751\n",
            "At-risk (threshold 0.5)? churn = 1\n",
            "\n",
            "Interpretation:\n",
            "- The churn probability is the model’s estimate of the chance that this customer will churn.\n",
            "  Example: 0.70 means about a 70% chance the customer will leave.\n",
            "- Businesses can use this to reduce churn by targeting high-risk customers (probability >= 0.5)\n",
            "  with retention actions like discounts, proactive support outreach, plan changes, or loyalty offers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Part 3\n",
        "# Data source: ChatGPT prompts (500 rows)\n",
        "# Data source file name: customer_segmentation_dataset\n",
        "# Columns: annual_spending, purchase_frequency, age, region\n",
        "\n",
        "# 1) Load dataset\n",
        "df = pd.read_csv(\"customer_segmentation_dataset.csv\")\n",
        "\n",
        "# 2) Basic checks / cleaning\n",
        "required_cols = [\"annual_spending\", \"purchase_frequency\", \"age\", \"region\"]\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "df = df.dropna(subset=required_cols)\n",
        "X = df[required_cols]\n",
        "\n",
        "# 3) Preprocessing: scale numeric + encode region\n",
        "numeric_features = [\"annual_spending\", \"purchase_frequency\", \"age\"]\n",
        "categorical_features = [\"region\"]\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numeric_features),                      # ✅ StandardScaler\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"), categorical_features)  # ✅ OneHotEncoder\n",
        "    ]\n",
        ")\n",
        "\n",
        "Xt = preprocess.fit_transform(X)\n",
        "\n",
        "# Convert to dense for speed (small dataset)\n",
        "Xt = Xt.toarray() if sparse.issparse(Xt) else Xt\n",
        "\n",
        "# 4) Elbow method (K=1..5) + plot inertia\n",
        "inertias = []\n",
        "k_range = range(1, 6)\n",
        "\n",
        "for k in k_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=5, max_iter=200)\n",
        "    km.fit(Xt)\n",
        "    inertias.append(km.inertia_)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(k_range), inertias, marker=\"o\")\n",
        "plt.xlabel(\"Number of clusters (K)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Elbow Method for K-Means (Customer Segmentation)\")\n",
        "plt.grid(True)\n",
        "\n",
        "elbow_path = \"elbow_plot.png\"\n",
        "plt.savefig(elbow_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "# 5) Fit final K-Means with K=3 (typical elbow for this synthetic dataset)\n",
        "K = 3\n",
        "kmeans_final = KMeans(n_clusters=K, random_state=42, n_init=10, max_iter=300)\n",
        "clusters = kmeans_final.fit_predict(Xt)\n",
        "\n",
        "# 6) Save cluster assignments to CSV\n",
        "df_out = df.copy()\n",
        "df_out[\"cluster\"] = clusters\n",
        "out_csv = \"customer_segmentation_with_clusters.csv\"\n",
        "df_out.to_csv(out_csv, index=False)\n",
        "\n",
        "# 7) Cluster analysis (means + region distribution)\n",
        "cluster_means = df_out.groupby(\"cluster\")[numeric_features].mean().round(2)\n",
        "cluster_region = pd.crosstab(df_out[\"cluster\"], df_out[\"region\"], normalize=\"index\").round(3)\n",
        "\n",
        "print(\"=== Part 3: Customer Segmentation (K-Means) ===\")\n",
        "print(\"\\nCluster Means (numeric):\")\n",
        "print(cluster_means)\n",
        "print(\"\\nRegion Distribution per Cluster (proportions):\")\n",
        "print(cluster_region)\n",
        "\n",
        "# 8) Suggested marketing strategies based on cluster patterns\n",
        "print(\"\\nSuggested Marketing Strategies:\")\n",
        "for c in cluster_means.index:\n",
        "    spend = cluster_means.loc[c, \"annual_spending\"]\n",
        "    freq = cluster_means.loc[c, \"purchase_frequency\"]\n",
        "\n",
        "    if spend > 8000 and freq > 25:\n",
        "        strategy = \"High-value loyalists: VIP rewards, early access, exclusive bundles.\"\n",
        "    elif spend > 3000 and freq > 12:\n",
        "        strategy = \"Core customers: personalized recommendations, points boosters, cross-sell.\"\n",
        "    else:\n",
        "        strategy = \"Budget/low-engagement: discounts, onboarding tips, reactivation campaigns.\"\n",
        "\n",
        "    print(f\"  Cluster {c}: {strategy}\")\n",
        "\n",
        "print(\"\\nSaved clustered dataset to:\", out_csv)\n",
        "print(\"Saved elbow plot to:\", elbow_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuDjxgCnsMEz",
        "outputId": "c43fa155-041c-4ca1-d862-fc6fd5bd6417"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Part 3: Customer Segmentation (K-Means) ===\n",
            "\n",
            "Cluster Means (numeric):\n",
            "         annual_spending  purchase_frequency    age\n",
            "cluster                                            \n",
            "0               12025.59               32.94  47.06\n",
            "1                2869.41               12.89  34.83\n",
            "2                3632.79               15.34  67.95\n",
            "\n",
            "Region Distribution per Cluster (proportions):\n",
            "region    East  North  South   West\n",
            "cluster                            \n",
            "0        0.222  0.361  0.222  0.194\n",
            "1        0.200  0.228  0.302  0.270\n",
            "2        0.254  0.249  0.282  0.215\n",
            "\n",
            "Suggested Marketing Strategies:\n",
            "  Cluster 0: High-value loyalists: VIP rewards, early access, exclusive bundles.\n",
            "  Cluster 1: Budget/low-engagement: discounts, onboarding tips, reactivation campaigns.\n",
            "  Cluster 2: Core customers: personalized recommendations, points boosters, cross-sell.\n",
            "\n",
            "Saved clustered dataset to: customer_segmentation_with_clusters.csv\n",
            "Saved elbow plot to: elbow_plot.png\n"
          ]
        }
      ]
    }
  ]
}