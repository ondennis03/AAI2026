{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXOQj2Pun+fKFh1qjCyMGL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ondennis03/AAI2026/blob/main/Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1 Prompt Chaining for a Customer Support AI:\n",
        "Step 1 (triage) → Step 2 (missing info questions) → Step 3 (resolution plan) → Step 4 (escalation decision)\n",
        "\n",
        "Iteration evidence: Step 2 includes V1 vs V2 prompt"
      ],
      "metadata": {
        "id": "dR3hbTTLtq5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools used: Google Colab + OpenAI API + ChatGPT"
      ],
      "metadata": {
        "id": "Tc9bZrrUuEfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai"
      ],
      "metadata": {
        "id": "VttOAFs27V5r"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "print(\"Key loaded:\", bool(os.environ.get(\"OPENAI_API_KEY\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewZlJPO97Xh5",
        "outputId": "24f59b95-3ba2-4226-dc81-0e86772f9e2e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key loaded: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Dict, Any\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def chat(prompt: str, model: str = \"gpt-4.1-mini\") -> str:\n",
        "    \"\"\"Send a single prompt and return the assistant's text.\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# Sample customer message\n",
        "\n",
        "CUSTOMER_MESSAGE = \"My order says delivered but I never got it. I need help ASAP.\"\n",
        "\n",
        "\n",
        "PROMPT_STEP1 = f\"\"\"\n",
        "You are a customer support triage assistant.\n",
        "\n",
        "Task: Classify the customer's issue into ONE category:\n",
        "{{billing, shipping/delivery, returns, technical, account}}.\n",
        "\n",
        "Output JSON only with keys:\n",
        "- category (one of the allowed categories)\n",
        "- urgency (integer 1-5)\n",
        "- reason (<= 20 words)\n",
        "\n",
        "Rules:\n",
        "- Do NOT propose a solution.\n",
        "- Do NOT ask questions.\n",
        "- Use only the customer's message.\n",
        "\n",
        "Customer message: \\\"\\\"\\\"{CUSTOMER_MESSAGE}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "step1_raw = chat(PROMPT_STEP1)\n",
        "print(\"STEP 1 OUTPUT (raw):\")\n",
        "print(step1_raw)\n",
        "\n",
        "step1 = json.loads(step1_raw)\n",
        "print(\"\\nSTEP 1 OUTPUT (parsed):\")\n",
        "print(step1)\n",
        "\n",
        "\n",
        "PROMPT_STEP2_V1 = f\"\"\"\n",
        "You are a customer support agent.\n",
        "\n",
        "Use this triage result:\n",
        "{json.dumps(step1)}\n",
        "\n",
        "Ask the customer for ONLY the missing info needed to resolve the issue.\n",
        "\n",
        "Constraints:\n",
        "- Friendly tone\n",
        "- Max 3 questions\n",
        "- No blaming language\n",
        "- Avoid policy jargon\n",
        "\n",
        "Output format:\n",
        "1) One short empathy sentence\n",
        "2) Numbered questions (1-3)\n",
        "\n",
        "Return only the formatted message.\n",
        "\"\"\"\n",
        "\n",
        "step2_v1 = chat(PROMPT_STEP2_V1)\n",
        "print(\"\\nSTEP 2 OUTPUT (V1):\")\n",
        "print(step2_v1)\n",
        "\n",
        "\n",
        "PROMPT_STEP2_V2 = f\"\"\"\n",
        "You are a customer support agent.\n",
        "\n",
        "Context:\n",
        "- Your job is to collect ONLY the minimum missing details needed to take the next action.\n",
        "- Do not request info the customer already provided.\n",
        "- Keep questions specific and easy to answer.\n",
        "\n",
        "Triage JSON:\n",
        "{json.dumps(step1)}\n",
        "\n",
        "Hard constraints:\n",
        "- Max 3 questions\n",
        "- Each question must be answerable in one line\n",
        "- No blaming (\"you should have...\"), no policy/legal language\n",
        "- Do NOT propose a solution yet\n",
        "- Tone: calm, helpful, urgent-appropriate\n",
        "\n",
        "Output format EXACTLY:\n",
        "<one empathy sentence>\n",
        "1) <question>\n",
        "2) <question>\n",
        "3) <question if needed; otherwise omit>\n",
        "\n",
        "Customer message (for reference):\n",
        "\\\"\\\"\\\"{CUSTOMER_MESSAGE}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "step2_v2 = chat(PROMPT_STEP2_V2)\n",
        "print(\"\\nSTEP 2 OUTPUT (V2 - improved):\")\n",
        "print(step2_v2)\n",
        "\n",
        "\n",
        "CUSTOMER_ANSWERS = {\n",
        "    \"order_number\": \"123456\",\n",
        "    \"delivery_address\": \"San Jose, CA 95112\",\n",
        "    \"carrier\": \"UPS\",\n",
        "    \"tracking_number\": \"1Z999AA10123456784\",\n",
        "    \"delivered_time\": \"Today 2:15 PM\",\n",
        "    \"delivery_notes_or_photo\": \"No photo/proof of delivery. No note from driver.\"\n",
        "}\n",
        "\n",
        "\n",
        "PROMPT_STEP3 = f\"\"\"\n",
        "You are a customer support agent.\n",
        "\n",
        "Goal:\n",
        "Provide a clear, helpful resolution plan for the customer.\n",
        "\n",
        "Inputs:\n",
        "Triage JSON:\n",
        "{json.dumps(step1)}\n",
        "\n",
        "Customer answers (JSON):\n",
        "{json.dumps(CUSTOMER_ANSWERS)}\n",
        "\n",
        "Constraints:\n",
        "- Provide a step-by-step plan (4-7 steps)\n",
        "- Include what YOU will do and what the CUSTOMER can do\n",
        "- Keep it concise and non-technical\n",
        "- Do not mention internal policies, legal language, or blame\n",
        "- End with a single sentence confirming the next update you will provide\n",
        "\n",
        "Output format:\n",
        "Title: <short title>\n",
        "Steps:\n",
        "1. ...\n",
        "2. ...\n",
        "...\n",
        "Closing: <one sentence>\n",
        "\"\"\"\n",
        "\n",
        "step3 = chat(PROMPT_STEP3)\n",
        "print(\"\\nSTEP 3 OUTPUT (Resolution plan):\")\n",
        "print(step3)\n",
        "\n",
        "\n",
        "PROMPT_STEP4 = f\"\"\"\n",
        "You are a support operations assistant.\n",
        "\n",
        "Decide whether to escalate this case to a human agent.\n",
        "\n",
        "Use:\n",
        "Triage JSON:\n",
        "{json.dumps(step1)}\n",
        "\n",
        "Proposed resolution plan:\n",
        "\\\"\\\"\\\"{step3}\\\"\\\"\\\"\n",
        "\n",
        "Escalation rules:\n",
        "- Escalate if urgency is 5 AND (high risk of loss, time-sensitive, or needs carrier claim/account access)\n",
        "- Otherwise do not escalate\n",
        "\n",
        "Output JSON only with keys:\n",
        "- escalate (true/false)\n",
        "- reason (<= 18 words)\n",
        "\"\"\"\n",
        "\n",
        "step4_raw = chat(PROMPT_STEP4)\n",
        "print(\"\\nSTEP 4 OUTPUT (Escalation decision):\")\n",
        "print(step4_raw)\n",
        "\n",
        "step4 = json.loads(step4_raw)\n",
        "print(\"\\nSTEP 4 OUTPUT (parsed):\")\n",
        "print(step4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "orz_CRFQ8PkY",
        "outputId": "31422c85-ad45-4027-c876-3f809a26c89f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1555/1352332854.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \"\"\"\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mstep1_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROMPT_STEP1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STEP 1 OUTPUT (raw):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep1_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1555/1352332854.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-4.1-mini\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m\"\"\"Send a single prompt and return the assistant's text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     resp = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[0;32m-> 1297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    }
  ]
}